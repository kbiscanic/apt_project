% Tenplate for TAR 2014
% (C) 2014 Jan Šnajder, Goran Glavaš
% KTLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2014}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{marvosym}
\usepackage{commath}

\title{Semantic textual similarity}

%VAŽNO: Zakomentirajte sljedeću liniju kada šaljete rad na recenziju
% karlo: i mailove
\name{Kristijan Biščanić, Karlo Dumbović, Nino Jagar} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{kristijan.biscanic,karlo.dumbovic,nino.jagar\}@fer.hr}\\
}

         
\abstract{ 
% This document provides the instructions on formatting the TAR project paper in \LaTeX. This is where you write the abstract (i.e., summary) of the work you carried out. The abstract is a paragraph of text ranging between 70 and 150 words.
This paper focuses on building a system that can assess the semantic similarity between two texts. Each text consists of only one sentence, so more sophisticated approach should be used than just simple word overlap. The sentences are first being preprocessed and then specific features are extracted. The features are being used to train a support vector regression model. The model outputs similarity score which is then compared with human similarity judgements. The performance of the model is being evaluated using Pearson and Spearman correlation coefficients.
}

\begin{document}

\maketitleabstract

\section{Introduction}

% This section is the introduction to your paper. Introduction should not be too elaborate, that is what other sections are for (the Introduction should definitely not spill over to second page). 

% This is the second paragraph of the introduction. Paragraphs are in \LaTeX separated by inserting an empty line in between them.  Avoid very large paragraphs (larger than half of the page height) but also avoid tiny paragraphs (e.g., one-sentence paragraphs).

Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation \citep{wiki_semantic}.

We have implemented a system that can measure the semantic similarity between two sentences whose context is previously unknown and of no matter to us. The idea of building such system comes from the necessity of automated similarity estimating in a number of study fields such as biomedical informatics, geoinformatics, linguistics and natural language processing (NLP).

The main task of the system is to compute certain features which act as a representation of sentence similarity. Each sentence is first preprocessed and then we compute various features from them. Exact features used are: ngram overlap, WordNet-augmented overlap, weighted word overlap, vector-space similarity, 2 types of normalized differences, shallow NERC and numbers overlap. Calculation of the features will be briefly described in forthcoming sections. These features are used as an input to a support vector regression (SVR) model. The model is being hyper-optimized by grid search over different values of hyper-parameters $C$, $\gamma$, $\epsilon$. On it's output, the model gives us the similarity judgement based on the features extracted. The similarity is scored as a real number from 0 to 5, where 0 represents no similarity, while 5 represents maximum similarity. Model estimations are compared with human judgements, and the accuracy of the model is measured using Pearson and Spearman correlation coefficients.

\section{Overview of the field}

% short overview of the field: similar algorithms and systems

Semantic textual similarity is an interesting field with an increasing interest in recent years. Various systems and algorithms are submitted in recent years for the Semeval/*SEM tasks organized in 2012, 2013 and 2014, with more than 60 participating teams \citep{semeval_web}. We designed our system for the SemEval-2012 Task 6 \citep{agirre2012semeval}.

One implementation of similar system is discussed at length in \citep{vsaric2012takelab}. The system described there directly inspired the implementation of our system.

\section{Description of the System}

We used a Support Vector Regression model (SVR) as our learning model. Our system first does a preprocessing step, and then we compute various features from preprocessed sentences.

\subsection{Preprocessing}
\label{sec:first}

To make our system more robust to small differences in inputs, we use the following preprocessing steps on each sentence:
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
  \item All dashes, brackets, slashes and hyphens are stripped;
  \item Various quotes are replaced with regular \textit{'} and \textit{"};
  \item Words are lowercased for calculation of most features;
  \item Names of currencies are stripped from the values, e.g., \textit{\EUR EUR100} becomes \textit{\EUR100};
  \item Words are tokenized using the pre-trained NLTK Punkt tokenizer;
  \item Tokens \textit{'m}, \textit{n't} and \textit{'re} are changed to \textit{am}, \textit{not} and \textit{are}, respectively;
  \item If a compound appears in one sentence, and it also appears in the other sentence but as two consecutive words, then they are replaced by that compound. E.g., \textit{foot ball} from first sentence will be replaced with word \textit{football} if it appears in the other sentence;
  \item Words are POS-tagged using the Maxent Treebank POS-tagger from NLTK;
  \item For calculation of some features, we removed stopwords using the NLTK stopwords corpus;
  \item We performed lemmatization for calculation of some features using the WordNet corpus from NLTK.
\end{enumerate}

\subsection{Ngram Overlap} 

The ngram overlap between two sentences is the harmonic mean of the degree to which the first sentence overlaps with the second and the degree to which the second sentence overlaps with the first.

First we compute $S_1$ and $S_2$, sets of consecutive ngrams from first and second sentence, respectively. The ngram overlap is then calculated using the following equation:

\begin{equation}
\mathit{overlap(S_1, S_2)} = 2 \cdot \frac{\abs{S_1 \cap S_2}}{\abs{S_1} + \abs{S_2}}
\end{equation}

Our system uses overlap scores based on unigrams, bigrams and trigrams. We calculated the overlap on both regular words and lemmas.

\subsection{Ostali featurei}

valjda ce stat svi? smijemo imat max 3 strane bez referenci

\section{Results}

\subsection{Model Training}

We used LIBSVM \citep{chang2011libsvm} to train a separate SVR model for each training set. The model was hyper-optimized (in terms of Pearson correlation) by grid search with nested cross-validation ($k=10$) to find the optimal parameters $C$, $g$ and $p$. Final prediction results are then trimmed to a 0-5 interval. For the surprise test set SMTnews we trained our system on SMTeuroparl train set, and for the OnWN test set we trained the system on the union of all provided train sets. The performance on train sets is shown in Table~\ref{tab:train-results-table}.

TODO - koji su dobri/losi featurei?

\begin{table}[h]
\caption{Cross-validated results on train sets}
\label{tab:train-results-table}
\begin{center}
\begin{tabular}{cccccc}
\toprule
Set & Pearson & Spearman & $C$ & $\gamma$ & $\epsilon$ \\
\midrule
\textit{MSRvid} & 1.0000 & 1.0000 & 1 & 1 & 1 \\
\textit{MSRpar} & 1.0000 & 1.0000 & 1 & 1 & 1 \\
\textit{SMTeuroparl} & 1.0000 & 1.0000 & 1 & 1 & 1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Test Set Results}

We evaluated our model using Pearson and Spearman correlation coefficients. The performance on test sets is shown in Table~\ref{tab:test-results-table}. Aggregate performance according to three aggregate evaluation measures proposed in \citep{agirre2012semeval} are shown in Table~\ref{tab:all-results-table}.

\begin{table}[h]
\caption{Performance on test sets}
\label{tab:test-results-table}
\begin{center}
\begin{tabular}{ccc}
\toprule
Set & Pearson & Spearman \\
\midrule
\textit{MSRvid} & 1.0000 & 1.0000 \\
\textit{MSRpar} & 1.0000 & 1.0000 \\
\textit{SMTeuroparl} & 1.0000 & 1.0000 \\
\textit{SMTnews} & 1.0000 & 1.0000 \\
\textit{OnWN} & 1.0000 & 1.0000 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\caption{Aggregate performance on test sets}
\label{tab:all-results-table}
\begin{center}
\begin{tabular}{ccc}
\toprule
& Pearson & Spearman \\
\midrule
\textit{ALL} & 1.0000 & 1.0000 \\
\textit{ALLnrm} & 1.0000 & 1.0000 \\
\textit{Mean} & 1.0000 & 1.0000 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Error analysis}

In order to analyse situations in which our system most likely fails, during every testing phase we created a file containing sentence pairs for which our model prediction and human judgement mostly differ. We discovered that there are few specific kinds of problems that we didn't manage to successfully overcome.

\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item If there are many words shared among sentences, but without much semantic similarity in contexts of these words, our system will predict the similarity score that is much bigger than expected. This kind of sentence pairs especially appear in \textit{MSRpar} and \textit{SMTeuroparl} datasets.
	
	\item On the other hand, if the sentences are semantically almost the same, but the words used in the sentences are not equal, our system will predict the score much lower than expected. This problem is most often present in \textit{MSRvid} dataset.
	
	\item Next interesting category of sentence pairs that cause our system to misjudge are pairs in which one sentence starts with another one and then adds some additional information. The result of this situation is that our system predicts the similarity score  severely lower than what it should be. This kind of sentences appear mostly in \textit{MSRvid} dataset.
\end{itemize}

\section{Conclusion and Future Work}

In this paper we presented our system for assessing the semantic textual similarity between two short texts based on machine learning. 

TODO

\nocite{*}
\bibliographystyle{tar2014}
\bibliography{tar2014} 

\end{document}

