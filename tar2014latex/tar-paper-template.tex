% Tenplate for TAR 2014
% (C) 2014 Jan Šnajder, Goran Glavaš
% KTLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2014}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{marvosym}
\usepackage{commath}

\title{Semantic textual similarity}

%VAŽNO: Zakomentirajte sljedeću liniju kada šaljete rad na recenziju
% karlo: i mailove
\name{Kristijan Biščanić, Karlo Dumbović, Nino Jagar} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{kristijan.biscanic,karlo.dumbovic,nino.jagar\}@fer.hr}\\
}

         
\abstract{ 
% This document provides the instructions on formatting the TAR project paper in \LaTeX. This is where you write the abstract (i.e., summary) of the work you carried out. The abstract is a paragraph of text ranging between 70 and 150 words.
This paper focuses on building a system that can assess the semantic similarity between two texts. Each text consists of only one sentence, so more sophisticated approach should be used than just simple word overlap. The sentences are first being preprocessed and then specific features are extracted. The features are being used to train a support vector regression model. The model outputs similarity score which is then compared with human similarity judgements. The performance of the model is being evaluated using Pearson and Spearman correlation coefficients.
}

\begin{document}

\maketitleabstract

\section{Introduction}

% This section is the introduction to your paper. Introduction should not be too elaborate, that is what other sections are for (the Introduction should definitely not spill over to second page). 

% This is the second paragraph of the introduction. Paragraphs are in \LaTeX separated by inserting an empty line in between them.  Avoid very large paragraphs (larger than half of the page height) but also avoid tiny paragraphs (e.g., one-sentence paragraphs).

Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation \citep{wiki_semantic}.

We have implemented a system that can measure the semantic similarity between two sentences whose context is previously unknown and of no matter to us. The idea of building such system comes from the necessity of automated similarity estimating in a number of study fields such as biomedical informatics, geoinformatics, linguistics and natural language processing (NLP).

The main task of the system is to compute certain features which act as a representation of sentence similarity. Each sentence is first preprocessed and then we compute various features from them. Exact features used are: ngram overlap, WordNet-augmented overlap, weighted word overlap, vector-space similarity, 2 types of normalized differences, shallow NERC and numbers overlap. Calculation of the features will be briefly described in forthcoming sections. These features are used as an input to a support vector regression (SVR) model. The model is being hyper-optimized by grid search over different values of hyper-parameters: regularization cost parameter $C$, RBF precision $\gamma$ and penalty threshold $\epsilon$. On its output, the model gives us the similarity judgement based on the features extracted. The similarity is scored as a real number from 0 to 5, where 0 represents no similarity, while 5 represents maximum similarity. Model estimations are compared with human judgements, and the accuracy of the model is measured using Pearson and Spearman correlation coefficients.

\section{Overview of the field}

% short overview of the field: similar algorithms and systems

Semantic textual similarity is an interesting field with an increasing interest in recent years. Various systems and algorithms are submitted in recent years for the Semeval/*SEM tasks organized in 2012, 2013 and 2014, with more than 60 participating teams \citep{semeval_web}. We designed our system for the SemEval-2012 Task 6 \citep{agirre2012semeval}.

One implementation of similar system is discussed at length in \citep{vsaric2012takelab}. The system described there directly inspired the implementation of our system.

\section{Description of the System}

We used a Support Vector Regression model (SVR) as our learning model. Our system first does a preprocessing step, and then we compute various features from preprocessed sentences.

\subsection{Preprocessing}
\label{sec:first}

To make our system more robust to small differences in inputs, we use the following preprocessing steps on each sentence:
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
  \item All dashes, brackets, slashes and hyphens are stripped;
  \item Various quotes are replaced with regular \textit{'} and \textit{"};
  \item Words are lowercased for calculation of most features;
  \item Names of currencies are stripped from the values, e.g., \textit{\EUR EUR100} becomes \textit{\EUR100};
  \item Words are tokenized using the pre-trained NLTK Punkt tokenizer;
  \item Tokens \textit{'m}, \textit{n't} and \textit{'re} are changed to \textit{am}, \textit{not} and \textit{are}, respectively;
  \item If a compound appears in one sentence, and it also appears in the other sentence but as two consecutive words, then they are replaced by that compound. E.g., \textit{foot ball} from first sentence will be replaced with word \textit{football} if it appears in the other sentence;
  \item Words are POS-tagged using the Maxent Treebank POS-tagger from NLTK;
  \item For calculation of some features, we removed stopwords using the NLTK stopwords corpus;
  \item We performed lemmatization for calculation of some features using the WordNet corpus from NLTK.
\end{enumerate}

\subsection{Ngram Overlap} 

The ngram overlap between two sentences is the harmonic mean of the degree to which the first sentence overlaps with the second and the degree to which the second sentence overlaps with the first.

First we compute $S_1$ and $S_2$, sets of consecutive ngrams from first and second sentence, respectively. The ngram overlap is then calculated using the following equation:

\begin{equation}
\mathit{overlap(S_1, S_2)} = 2 \cdot \frac{\abs{S_1 \cap S_2}}{\abs{S_1} + \abs{S_2}}
\end{equation}

Our system uses overlap scores based on unigrams, bigrams and trigrams. We calculated the overlap on both regular words and lemmas.

\subsection{Weighted word overlap}

First, for every word $w$ in corpus $C$ we have to compute its information content $ic$ using the following equation:
\begin{equation}
	ic(w) = \ln \frac{\sum_{w' \in C} freq(w')}{freq(w)}
\end{equation}
where $freq(w)$ is the word frequency that we obtain indirectly from Google Books Ngrams, whose coverage for English is extremely good. \citep{michel2011quantitative}

Then, we introduce the weighted word coverage of the second sentence by the first one, which is calculated using the following equation:
\begin{equation}
	wwc(S_1, S_2) = \frac{\sum_{w \in S_1 \cap S_2} ic(w)}{\sum_{w \in S_2} ic(w)}
\end{equation}
where $S_1$ and $S_2$ are sets of words appearing in the sentences.

Finally, the weighted word overlap is calculated as a harmonic mean of the $wwc(S_1,S_2)$ and $wwc(S_2,S_1)$.

\subsection{Normalized differences}

Normalized difference for two integers $a$ and $b$ is calculated as
\begin{equation}
	nd(a, b) = \frac{|a - b|}{\max \{a, b\}}.
\end{equation}
In our system, we use two types of normalized differences. The first one takes as input sentence lengths, while the second one uses aggregate information content of the words in sentences.

\subsection{Numbers overlap}

Inspired by \citep{socher2011dynamic}, we decided to compute few features which will represent numbers overlap. To begin, we need to extract the numbers from each sentence in separate sets $N_1$ and $N_2$.
The features we observe are computed as follows:
\begin{align}
	no^{(1)} &= \ln (1+|N_1|+|N_2|) \\
	no^{(2)} &= \frac{2 |N_1 \cap N_2|}{|N_1| + |N_2|} \\
	no^{(3)} &= (N_1 \subset N_2) \vee (N_2 \subset N_1)
\end{align}
As proposed in \citep{vsaric2012takelab}, numbers that differ only in the number of decimal places are treated as equal.

\subsection{WordNet-Augmented Word Overlap}
The \textit{WordNet-augmented word overlap} feature is defined as a harmonic mean of \textit{WordNet-augmented coverages} $P_{WN}(S_1, S_2)$ and $P_{WN}(S_2, S_1)$. $P_{WN}(S_1, S_2)$ is computed as:
\begin{equation}
P_{WN}(S_1, S_2) = \dfrac{1}{|S_2|}\sum_{w \in S_1}score(w, S_2)
\end{equation}
\begin{equation}
score(w, S) = \left\{
\begin{array}{ll}
1 & \mbox{if } w \in S \\
\underset{w' \in S}{\max} \: sim(w, w') & \mbox{otherwise}
\end{array}
\right.
\end{equation}
$sim(w, w')$ represents the WordNet path length similarity of words $w$ and $w'$. We use this feature to find unigram overlap between sentences that are semantically similar but don't use exactly the same words. We are using WordNet to assign partial scores to these words and thus allowing for some lexical variation.

\subsection{Vector Space Sentence Similarity}
We represent each sentence in the vector space as a single vector $u$. $u$ is calculated by summing the LSA vector of each word in the sentence:
\begin{equation}
u(S) = \sum_{w \in S} \textbf{x}_w
\end{equation}
$\textbf{x}_w$ is a word vector taken from pre-trained word vectors file that was trained by \textit{word2vec} tool for computing continuous distributed representation of words on part of Google News dataset. After calculating the representations of both sentences we define our feature as:
\begin{equation}
vsss(S_1, S_2) = |\cos(u(S_1), u(S_2))|
\end{equation}

Another similar vector space representation $u_W$ uses the information content $ic(w)$ to weigh LSA vector in the following way:
\begin{equation}
u_W(S) = \sum_{w \in S} ic(w)\textbf{x}_w
\end{equation}
We define the second feature as:
\begin{equation}
vsss'(S_1, S_2) = |\cos(u_W(S_1), u_W(S_2))|
\end{equation}

\subsection{Shallow Named Entity Recognizer and Classifier}
Shallow named entity recognizer and classifier treats every capitalized word as a named entity if it is longer than one character and classifies every word written in all caps and begging with a period as a stock index symbol. Our system uses four features associated with named entities: the overlap of named entities, the feature indicating if named entities were found in either of the sentences, the overlap of stock index symbols and the feature indicating if stock index symbols were found in either of the sentences.

\section{Results}

\subsection{Model Training}

We used LIBSVM \citep{chang2011libsvm} to train a separate SVR model for each training set. The model was hyper-optimized (in terms of Pearson correlation) by grid search with nested cross-validation ($k=10$) to find the optimal parameters $C$, $\gamma$ and $\epsilon$. Final prediction results are then trimmed to a $[0,5]$ interval. For the surprise test set \textit{SMTnews} we trained our system on \textit{SMTeuroparl} train set, and for the \textit{OnWN} test set we trained the system on the union of all provided train sets. The performance on train sets is shown in Table~\ref{tab:train-results-table}.

TODO - koji su dobri/losi featurei?

\begin{table}[h]
\caption{Cross-validated results on train sets}
\label{tab:train-results-table}
\begin{center}
\begin{tabular}{cccccc}
\toprule
Set & Pearson & Spearman & $C$ & $\gamma$ & $\epsilon$ \\
\midrule
\textit{MSRvid} & 0.8924 & 0.8894 & 64 & 0.25 & 0.4 \\
\textit{MSRpar} & 0.7398 & 0.6663 & 32768 & 0.00049 & 0.6 \\
\textit{SMTeuroparl} & 0.8191 & 0.6713 & 16 & 0.25 & 0.05 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Test Set Results}

We evaluated our model using Pearson and Spearman correlation coefficients. The performance on test sets is shown in Table~\ref{tab:test-results-table}. Aggregate performance according to three aggregate evaluation measures proposed in \citep{agirre2012semeval} are shown in Table~\ref{tab:all-results-table}.

\begin{table}[h]
\caption{Performance on test sets}
\label{tab:test-results-table}
\begin{center}
\begin{tabular}{ccc}
\toprule
Set & Pearson & Spearman \\
\midrule
\textit{MSRvid} & 0.87926 & 0.87531 \\
\textit{MSRpar} & 0.66632 & 0.64689 \\
\textit{SMTeuroparl} & 0.37214 & 0.40216 \\
\textit{SMTnews} & 0.54115 & 0.28306 \\
\textit{OnWN} & 0.69060 & 0.66542 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\caption{Aggregate performance on test sets}
\label{tab:all-results-table}
\begin{center}
\begin{tabular}{ccc}
\toprule
& Pearson & Spearman \\
\midrule
\textit{ALL} & 0.79688 & 0.76381\\
\textit{ALLnrm} & 0.86295 & 0.79843 \\
\textit{Mean} & 0.67035 & 0.62363 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Error analysis}

In order to analyse situations in which our system most likely fails, during every testing phase we created a file containing sentence pairs for which our model prediction and human judgement mostly differ. We discovered that there are few specific kinds of problems that we didn't manage to successfully overcome.

\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item If there are many words shared among sentences, but without much semantic similarity in contexts of these words, our system will predict the similarity score that is much bigger than expected. This kind of sentence pairs especially appear in \textit{MSRpar} and \textit{SMTeuroparl} datasets. To solve this problem we should try to add more features describing semantic relationships between words, rather than their individual meanings.
	
	\item On the other hand, if the sentences are semantically almost the same, but the words used in the sentences are not equal, our system will predict the score much lower than expected. This problem is most often present in \textit{MSRvid} dataset. The approach that would give us better results when it comes to this kind of situations could consist of removing some features that focus directly on comparing word equality and add other ones that take into account their semantic meaning.
	
	\item Next interesting category of sentence pairs that cause our system to misjudge are pairs in which one sentence starts with another one and then adds some additional information. The result of this situation is that our system predicts the similarity score  severely lower than what it should be. This kind of sentences appear mostly in \textit{MSRvid} dataset.
\end{itemize}

\section{Conclusion and Future Work}

In this paper we presented our system for assessing the semantic textual similarity between two short texts based on machine learning. This system was designed as a solution to the Text Analysis and Retrieval project assignment. The whole system was written in Python, utilizing several libraries such as NLTK, NumPy, scikit-learn and LIBSVM. Training and testing of the system was done on datasets from the SemEval-2012 Semantic Textual Similarity Task. Performance of the system was evaluated using Pearson and Spearman correlations with human similarity judgements and, according to our scores, would be ranked 4th on the said competition.

System is performing very well on three of five test sets and there is plenty of room for improvement. There certainly exist features that could be implemented and would possibly improve performance of this system. Some of these features are full NERC, syntactic roles similarity, syntactic dependencies overlap, greedy lemma aligning overlap, semantic role labelling, etc.

It could also be possible to obtain better results using current features by using larger grid for more detailed grid search but it would be very computationally expensive and it would take several times more time to train a model on the same dataset.

\nocite{*}
\bibliographystyle{tar2014}
\bibliography{tar2014} 

\end{document}

